{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # Finetuning Falcon 7B the first commercially opensource LLM on an opensource instruction dataset."
      ],
      "metadata": {
        "id": "5IFPfsoApc5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recent papers are showing the ability to train smaller models to outperform GPT-4. Notably recent research papers have demonstrated the ability to finetune smaller 7B models some highlights include: \n",
        "\n",
        "*   **Gorilla** -  LLM Connected with Massive APIs- https://arxiv.org/abs/2305.15334\n",
        "*   **Goat** - able to surpass GPT-4 on Arithmetic Tasks https://arxiv.org/pdf/2305.14201v1.pdf\n",
        "\n",
        "Other recent papers also suggest that a quality small dataset can help smaller language models shine.\n",
        "\n",
        "*   **LIMA** - less is more for alignment. https://arxiv.org/pdf/2305.11206.pdf\n",
        "\n",
        "With the advancements announced in the QLora paper it is now possible to finetune smaller models on one GPU.\n",
        "\n",
        "*   **QLoRA** https://arxiv.org/pdf/2305.14314v1.pdf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4q4tOB_ipt6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Below is code on how you can finetune an LLM (in this case Falcon released by the UAE for free commercial use) on instruction data to achieve state of the art results on specific tasks. This code implements QLoRA so is capable of running on a single gpu with <24gb vram.\n",
        "\n",
        "**As recent paper show the most important part is not the size of the model but the dataset you are training it on.**"
      ],
      "metadata": {
        "id": "oPLssiEQra8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqPYMutlXFmu"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install einops\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git \n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj3bBAFBZxmZ",
        "outputId": "656a433f-2d39-41c3-edd4-f58c7fdcf371"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jun  2 09:24:02 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    27W /  70W |   6793MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"tiiuae/falcon-7b\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, trust_remote_code=True, device_map={\"\":0})"
      ],
      "metadata": {
        "id": "l8CpKUCDXN_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Prepare for peft training (parameter efficient fine tuning)</h1>"
      ],
      "metadata": {
        "id": "C7DFt9GEXbXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "aSRF17jmXaVt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "D79ii0NRl8Sq"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8, \n",
        "    lora_alpha=32, \n",
        "    target_modules=[\"query_key_value\"], \n",
        "    lora_dropout=0.05, \n",
        "    bias=\"none\", \n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RquYNWeNl2MC",
        "outputId": "0eb55523-f421-45f3-bbb3-bc506de474d4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2359296 || all params: 3611104128 || trainable%: 0.06533447711203746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load our commercially opensource instruction dataset (Dolly 15k)"
      ],
      "metadata": {
        "id": "7vlfopuroIMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"databricks/databricks-dolly-15k\")"
      ],
      "metadata": {
        "id": "QXYB68GRakEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDk6qGjCbkXk",
        "outputId": "ffbb4022-43b3-4eb8-a75d-90ea9f42d042"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'context', 'response', 'category'],\n",
              "        num_rows: 15011\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the dataset by tokenizing and also joining some of the columns"
      ],
      "metadata": {
        "id": "uAblZRiPo4Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    # Concatenate instruction and input text\n",
        "    input_text = [' '.join(t) for t in zip(examples[\"instruction\"], examples[\"context\"])]\n",
        "    output_text = examples[\"response\"]\n",
        "\n",
        "    # Tokenize inputs and outputs\n",
        "    inputs = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=512)\n",
        "    outputs = tokenizer(output_text, padding=\"max_length\", truncation=True, max_length=512)\n",
        "    \n",
        "    return {\"input_ids\": inputs.input_ids, \"attention_mask\": inputs.attention_mask, \"labels\": outputs.input_ids}\n",
        "\n",
        "dataset = data.map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"context\", \"response\"])"
      ],
      "metadata": {
        "id": "WgZyOW25cOzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ],
      "metadata": {
        "id": "essiQx04iIFm"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMsx0DZjiwTc",
        "outputId": "7a836201-8b3e-4080-e11c-3a09845efbdc"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['category', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 15011\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the dataset up into train and test"
      ],
      "metadata": {
        "id": "_NszDl7Mon80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Split the data into 80% for training and 20% for evaluation\n",
        "split_data = dataset['train'].train_test_split(test_size=0.2)\n",
        "\n",
        "# Now we update the dataset with the new split data\n",
        "dataset = DatasetDict({\n",
        "    'train': split_data['train'],\n",
        "    'eval': split_data['test']\n",
        "})\n"
      ],
      "metadata": {
        "id": "rvkY9elXi46H"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "hf9nRcappThv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per bitsandbytes example i have set this to run for 10 steps as a tester. If you uncomment out the **num_train_epochs** this will train properly, and based on the ETA would finetune in about 24 hours."
      ],
      "metadata": {
        "id": "pROarGdzs53J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=2,\n",
        "        # num_train_epochs=3,\n",
        "        max_steps=10,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['eval'],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "-ja-ESroiVSb",
        "outputId": "577cd0f6-588d-48b8-f253-8196fa93f034"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 01:33, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.115400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.343800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.544700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.564300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.495900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.927200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.640500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.759200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.656900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=1.6072865188121797, metrics={'train_runtime': 103.5639, 'train_samples_per_second': 0.386, 'train_steps_per_second': 0.097, 'total_flos': 407425237647360.0, 'train_loss': 1.6072865188121797, 'epoch': 0.0})"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    }
  ]
}